<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jason Wu</title>

    <meta name="author" content="Wu Chi Hsuan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wu Chi Hsuan (Jason)
                </p>
                <p>
                  I am currently a research assistant at the <a href="https://www.iis.sinica.edu.tw/en/page/ResearchGroup/MultimediaTechnologies.html">Academia Sinica Multi-Media Technologies Lab</a> supervised by <a href="https://sites.google.com/site/jenchunlin/">Prof. Lin, Jen Chun</a>. I graduated from Hong Kong University of Science and Technology in 2023. During university, I worked on my Final Year Thesis supervised by <a href="https://vsdl.hkust.edu.hk/people.html">Prof. Kwang-Ting Cheng</a>. I also worked as a Research Assistant in the <a href="https://indy.epfl.ch/">Information and Network Dynamics Lab</a> at EPFL supervised by <a href="https://people.epfl.ch/matthias.grossglauser">Prof. Matthias Grossglauser</a> during exchange.
                </p>
                <p style="text-align:center">
                  <a href="wuchihsuan.working@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/WuChiHsuan-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://medium.com/@wuch9015">Medium</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JasonWUCHI">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/jasons_face.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Interest</h2>
                <p>
                  I am interested in Human Poses and Behavioral Analysis using interaction between different modalities. Human behavior can be estimated from images or videos using poses and deep features, while other modalities can also be informative. For instance, can we improve the prediction of a personâ€™s behavior using the speech and expressions of surrounding people? Can we generate text instructions to correct incorrectly performed movements, such as dancing? Can behavior classification and generation be personalized? These research questions allow behavior analysis insight to be transferred into understandable texts. They also connect prior behavioral studies with applications.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="mocorank/teaser.png" alt="mocorank" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
              
                <span class="papertitle">CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels</span>
                
                <br>
                <strong>Wu Chi hsuan</strong>, Liu Shih Yang, Huang Xijie, Prof. Tim Kwang-Ting Cheng
                <br>
                <em>Submitted to AAAI AI for Education Workshop</em>, 2024
                <p>We designed a multi-modal model and a contrastive ranking loss to detect student engagement in online classes. Collaborated with LifeHikes.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="biasometer/teaser.png" alt="biasometer" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                
                <a href="https://arxiv.org/abs/2307.08139">
                  <span class="papertitle">It's All Relative: Interpretable Models for Scoring Bias in Documents</span>
                </a>
                <br>
                  Aswin Suresh, <strong>Wu Chi hsuan</strong>, Prof. Matthias Grossglauser
                <br>
                <em>Submitted to The European Chapter of the ACL.</em>, 2024
                <p>We designed a interpretable model structure to detect bias articles and identify subjectivity level of each term based on the context.</p>
              </td>
            </tr>

    <!-- <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        <a href="https://keunhong.com/">Keunhong Park</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Jonathan T. Barron</strong>,
        <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr> 

    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</span>
          </a>
          <br>
          <strong>Jonathan T. Barron</strong>,
          <a href="https://bmild.github.io/">Ben Mildenhall</a>,
          <a href="https://scholar.harvard.edu/dorverbin/home">Dor Verbin</a>,
          <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
          <a href="https://phogzone.com/">Peter Hedman</a>
          <br>
          <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
          <br>
          <a href="http://jonbarron.info/zipnerf">project page</a>
          /
          <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
          /
          <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
          <p></p>
          <p>
          Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x.
          </p>
        </td>
      </tr>-->

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Other Projects</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="other_project/CycleGAN_teaser.png" alt="CycleGAN" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
              
                <span class="papertitle">Day-Night-Transfomation-for-improving-feature-matching</span>
                
                <br>
                  <em>Computer Vision Project</em> <a \href="https://github.com/JasonWUCHI/Day-Night-Transfomation-for-improving-feature-matching"> [Github]  </a>
                <br>
                <p>We transformed illumination of day-night image pairs using CycleGAN to improve feature matching.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="other_project/SR_teaser.png" alt="SR" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                
                <span class="papertitle">Super-Resolution on Computer Texts</span>
                
                <br>
                  Undergraduate Research Opportunity Program, Supervised by Prof. Qifeng Chen
                <br>
                <a href="https://github.com/JasonWUCHI/TSRN-SR-Experiment-on-Computer-Text">[Github]</a>
                <p>We designed two-stream model to simultaneuously improve text boundary clarity and colors on text images.</p>
              </td>
            </tr>
          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>             -->

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://medium.com/@wuch9015/%E9%A6%99%E6%B8%AF%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%B8%E8%A8%98%E5%AF%A6-%E5%A4%A7%E4%B8%80%E4%B8%8A-7ab2a6e949e">Series of Life Recording: Hong Kong Universiy of Science and Technology</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to use this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
