<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="n">
  <meta name="keywords" content="cmose, engagement, dataset, Multi-Modality">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels</title>

  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset
            with High-Quality Labels</h2>
          <h2 class="is-size-4 publication-authors"> CVPR, ABAW Workshop 2024</h2><br>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://jasonwuchi.github.io/">Chi Hsuan Wu</a><sup>1</sup>,</span>
            <span class="author-block">
                Shih-yang Liu<sup>1</sup>,</span>
            <span class="author-block">
                Xijie Huang<sup>1</sup>,</span>
            <span class="author-block">
                Xingbo Wang<sup>1</sup>,</span>
            <span class="author-block">
                Rong Zhang<sup>1</sup>,</span>
            <span class="author-block">
              Luca Minciullo<sup>2</sup>,</span>
            <span class="author-block">
              Wong Kai Yiu<sup>2</sup>,</span>
            <span class="author-block">
              Kenny Kwan<sup>2</sup>,</span>
            <span class="author-block">
              Kwang-Ting Cheng<sup>1</sup>,</span>
            </span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup> HKUST</span>&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup> LifeHikes</span><br>&nbsp;&nbsp;

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024W/ABAW/papers/Wu_CMOSE_Comprehensive_Multi-Modality_Online_Student_Engagement_Dataset_with_High-Quality_Labels_CVPRW_2024_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/CVPR2024W/ABAW/supplemental/Wu_CMOSE_Comprehensive_Multi-Modality_CVPRW_2024_supplemental.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplemental Material</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/xxxx.xxxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="https://github.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Hugging Face</span>
                  </a>
              </span> -->

              <span class="link-block">
                <a href="https://huggingface.co/datasets/cwuau/CMOSE"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                        alt="Hugging Face " style="height:1.2em;">
                  </span>
                  <span>Hugging Face (Dataset)</span>
                </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <p><strong>Abstract</strong>: Online learning is a rapidly growing industry. However, a major doubt about online learning is whether students are as engaged as they are in face-to-face classes. An engagement recognition system can notify the instructors about the student's condition and improve the learning experience. Current challenges in engagement detection involve poor label quality, extreme data imbalance, and intra-class variety - the variety of behaviors at a certain engagement level. To address these problems, we present the CMOSE dataset, which contains a large number of data from different engagement levels and high-quality labels annotated according to psychological advice. We also propose a training mechanism MocoRank to handle the intra-class variety and the ordinal pattern of different degrees of engagement classes. MocoRank outperforms prior engagement detection frameworks, achieving a 1.32% increase in overall accuracy and 5.05% improvement in average accuracy. Further, we demonstrate the effectiveness of multi-modality in engagement detection by combining video features with speech and audio features. The data transferability experiments also state that the proposed CMOSE dataset provides superior label quality and behavior diversity.</p>

              <!-- Paper video.
        <div class="columns is-centered has-text-centered video-container">
          <div class>
            <h2 class="title is-3">Supplementary video</h2>
              <video width="1000" height="100%" controls>
                <source src="https://youtu.be/zo8jGfex3gQ" type="video/mp4">
              </video>
          </div>
      </div> -->

      <div class="columns is-centered has-text-centered video-container">
        <div class="column">
          <h2 class="title is-3">Supplementary video</h2>
          <div class="video-responsive">
            <iframe
              width="900"
              height="562"
              src="https://www.youtube.com/embed/zo8jGfex3gQ"
              title="Supplementary video"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Wu_2024_CVPR,
      author={Wu, Chi-Hsuan and Liu, Shih-Yang and Huang, Xijie and Wang, Xingbo and Zhang, Rong and Minciullo, Luca and Yiu, Wong Kai and Kwan, Kenny and Cheng, Kwang-Ting},
      title={CMOSE: Comprehensive Multi-Modality Online Student Engagement Dataset with High-Quality Labels},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
      month={June},
      year={2024},
      pages={4636-4645}
  }
  </code></pre>
  </div>
</section>

</body>
</html>
